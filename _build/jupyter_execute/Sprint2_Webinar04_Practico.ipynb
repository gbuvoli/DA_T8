{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d398fc7",
   "metadata": {
    "id": "0d398fc7"
   },
   "source": [
    "# Sprint 2 - Webinar 4: Walmart Project — Limpieza, Enriquecimiento y Resumen con KPIs (80 min)\n",
    "\n",
    "**Creado:** 2025-09-14 01:23  \n",
    "**Cobertura:** Solo **mitad de los ítems** del proyecto → **Partes 1–3** (dejamos Dashboard, Resumen Ejecutivo y QA para la Parte 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972812a3",
   "metadata": {
    "id": "972812a3"
   },
   "source": [
    "---\n",
    "## Objetivos de la sesión\n",
    "- **LIMPIAR**: normalizar nombres, estandarizar fechas por semana (`semana_limpia`).\n",
    "- **ENRIQUECER**: unir `raw_ventas` con `stores` y `departments` vía `XLOOKUP/VLOOKUP`.\n",
    "- **RESUMIR**: construir pivots para **KPI de eficiencia (ventas/m²)**, **participación** y **volatilidad (CV)**.\n",
    "- Documentar supuestos y validaciones básicas del merge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fda1f",
   "metadata": {
    "id": "896fda1f"
   },
   "source": [
    "---\n",
    "\n",
    "## Material\n",
    "- Metodología: Breakout Rooms\n",
    "- Kahoot: W4_Sprint2 (finalizando sesión)\n",
    "\n",
    "## Agenda (80 min, práctica)\n",
    "**Bienvenida + Introducción**\n",
    "1) Setup y descripción del dataset (5 min)  \n",
    "2) **Parte 1** — Limpieza de datos (25–30 min)  \n",
    "3) **Parte 2** — Enriquecimiento (20–25 min)  \n",
    "4) **Parte 3** — Resumen con KPIs vía tablas dinámicas (20–25 min)  \n",
    "> **No cubrimos hoy**: Dashboard, Resumen C→F→I, QA (se verán en la próxima clase).\n",
    "\n",
    "## Cierre\n",
    "\n",
    "- Kahoot\n",
    "- Repaso de aprendizajes y próximos pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d8718",
   "metadata": {
    "id": "384d8718"
   },
   "source": [
    "---\n",
    "## 0) Setup (descarga y vista previa)\n",
    "Descarga los archivos de datos (úsalos en Excel/Sheets):\n",
    "- [walmart_sales.csv](/mnt/data/walmart_sales.csv)\n",
    "- [walmart_stores.csv](/mnt/data/walmart_stores.csv)\n",
    "- [walmart_departments.csv](/mnt/data/walmart_departments.csv)\n",
    "- **(Opcional)**: [walmart_project_data.xlsx](/mnt/data/walmart_project_data.xlsx) — ya con hojas: `raw_ventas`, `stores`, `departments`.\n",
    "\n",
    "> Si usas Python para mostrar ejemplos en vivo, ejecuta la próxima celda para una vista previa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d453b2",
   "metadata": {
    "id": "19d453b2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/walmart_sales.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 4\u001b[0m sales \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/walmart_sales.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m stores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/walmart_stores.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m depts  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/walmart_departments.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/walmart_sales.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "sales = pd.read_csv(r\"/mnt/data/walmart_sales.csv\")\n",
    "stores = pd.read_csv(r\"/mnt/data/walmart_stores.csv\")\n",
    "depts  = pd.read_csv(r\"/mnt/data/walmart_departments.csv\")\n",
    "\n",
    "sales.head(3), stores.head(3), depts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447b827",
   "metadata": {
    "id": "0447b827"
   },
   "source": [
    "---\n",
    "# Parte 1 — Limpieza de datos (20–30 min)\n",
    "\n",
    "**Hoja de trabajo:** duplica `raw_ventas` → nómbrala `clean_ventas` (marca pestaña en **verde**).\n",
    "\n",
    "### 0. Break rooms:\n",
    "\n",
    "Identifica qué cambios harías a la hoja de cáculo.\n",
    "-  Cambiarias nombres de columnas?\n",
    "- Agregarías más columnas?\n",
    "- Cambiarias algún valor de las columnas? Quizás los 0/1\n",
    "\n",
    "### 1.1 Normaliza columnas\n",
    "- Renombra a **snake_case** o nombres declarativos: `tienda | dept | fecha | ventas_semanales | es_feriado`\n",
    "- Aplica:\n",
    "  - `=TRIM()` para eliminar espacios sobrantes\n",
    "  - `=LOWER()/UPPER()/PROPER()` para estandarizar texto\n",
    "  - Validación de datos si corresponde (`es_feriado` ∈ {0,1})\n",
    "\n",
    "### 1.2 Estandariza semana (clave temporal)\n",
    "Crea columna **`semana_limpia`** (año-semana ISO; lunes como primer día):\n",
    "\n",
    "- **Excel (semana ISO aproximada)**  \n",
    "  `=YEAR([@fecha]) & \"-\" & TEXT(WEEKNUM([@fecha], 2), \"00\")`\n",
    "\n",
    "- **Google Sheets**  \n",
    "  `=YEAR(C2) & \"-\" & TEXT(WEEKNUM(C2, 2), \"00\")`\n",
    "\n",
    "> Comprueba que todas las filas sean 2012. Si encuentras otras fechas, documenta el criterio (filtrar/ajustar).\n",
    "\n",
    "**Salida esperada**: hoja `clean_ventas` lista para análisis:\n",
    "`tienda | dept | fecha | ventas_semanales | es_feriado | semana_limpia`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827210e7",
   "metadata": {
    "id": "827210e7"
   },
   "source": [
    "---\n",
    "# Parte 2 — Enriquecimiento (20–30 min)\n",
    "**Objetivo**: añadir `tipo`, `tamaño`, `nombre_dept` a `clean_ventas` desde `stores` y `departments`.\n",
    "\n",
    "### 2.1 Join por `Store` → `Type` y `Size`\n",
    "En `clean_ventas`, inserta columnas:\n",
    "- `tipo` (desde `stores[Type]`)\n",
    "- `tamaño` (desde `stores[Size]`)\n",
    "\n",
    "**Google Sheets — con `XLOOKUP`**  \n",
    "`=XLOOKUP(A2, Stores!A:A, Stores!B:B, \"NoMatch\", 0))`\n",
    "\n",
    "*(Sheets usa la misma sintaxis de XLOOKUP; con VLOOKUP, recuerda el índice de columna y `FALSE`)*\n",
    "\n",
    "### 2.2 Join por `Dept` → `dept_name`\n",
    "`=XLOOKUP(B2, Departments!A:A, Departments!B:B, \"NoMatch\", 0)` → `nombre_dept`\n",
    "\n",
    "### 2.3 Verificación y troubleshooting\n",
    "- Cuenta no coincidencias: `=COUNTIF(RangoResultados,\"NoMatch\")`\n",
    "- Revisa duplicados en claves (`Store`, `Dept`) según tu diseño de unión.\n",
    "- Documento “JOIN_DOC”:\n",
    "  - Claves y cardinalidad asumidas\n",
    "  - % de `NoMatch`\n",
    "  - Decisiones tomadas ante errores (corrección, exclusión, imputación)\n",
    "\n",
    "**Salida esperada**: `tienda | dept | fecha | ventas_semanales | es_feriado | semana_limpia | tipo | tamaño | nombre_dept`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041fc37",
   "metadata": {
    "id": "e041fc37"
   },
   "source": [
    "---\n",
    "# Parte 3 — Resumen con KPIs vía Tablas Dinámicas (20–30 min)\n",
    "\n",
    "> Construye **una pivot por KPI** y deja los cálculos como **campos calculados** cuando aplique.\n",
    "\n",
    "## KPI 1 — Eficiencia (Ventas por m²)\n",
    "1) Pivot (por `tienda` o `nombre_dept`):  \n",
    "   - Valores: **SUM(ventas_semanales)** y **AVERAGE(tamaño)**\n",
    "2) Campo calculado **`ventasxmetro2`**:  \n",
    "   `=SUM(ventas_semanales) / AVERAGE(tamaño)`\n",
    "3) Ordena de mayor a menor.\n",
    "\n",
    "**Salida**: columnas → `nombre_dept` | `SUM ventas` | `AVG tamaño` | `ventasxmetro2`\n",
    "\n",
    "---\n",
    "\n",
    "## KPI 2 — Participación del departamento (% sobre total)\n",
    "1) Pivot por `nombre_dept` con **SUM(ventas_semanales)**  \n",
    "2) Mostrar valor como **% del total**.\n",
    "\n",
    "**Salida**: `nombre_dept` | `% del total de ventas`\n",
    "\n",
    "---\n",
    "\n",
    "## KPI 3 — Volatilidad (Coeficiente de Variación, CV)\n",
    "1) Pivot por `nombre_dept` con:\n",
    "   - `SUM(ventas_semanales)`\n",
    "   - `STDEV(ventas_semanales)`\n",
    "   - `AVERAGE(ventas_semanales)`\n",
    "2) Campo calculado **`cv`**:  \n",
    "   `=STDEV(ventas_semanales) / AVERAGE(ventas_semanales)`\n",
    "\n",
    "**Interpretación**:  \n",
    "- CV ~ 0 → estable; CV ≥ 0.5 → volátil; CV ≥ 1 → muy volátil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3d0c1",
   "metadata": {
    "id": "b9b3d0c1"
   },
   "source": [
    "---\n",
    "## Entregables al cierre de la sesión (Parte 1 de 2)\n",
    "- Hoja **`clean_ventas`** lista.\n",
    "- Hoja **`JOIN_DOC`** con supuestos y verificaciones (conteo de `NoMatch`, duplicados).\n",
    "- Tres pivots, uno por KPI (**eficiencia, participación, volatilidad**) con campos calculados.\n",
    "- 2–3 bullets de hallazgos rápidos (sin dashboard todavía)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1366f1",
   "metadata": {
    "id": "cd1366f1"
   },
   "source": [
    "---\n",
    "## Criterios de evaluación rápidos\n",
    "- **Limpieza**: nomenclatura consistente y `semana_limpia` correcta.  \n",
    "- **Enriquecimiento**: `XLOOKUP/VLOOKUP` correctos, verificación documentada.  \n",
    "- **KPIs**: campos calculados y pivots claros, ordenados y legibles.  \n",
    "- **Razonamiento**: Justificación de decisiones ante errores y outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96482d71",
   "metadata": {
    "id": "96482d71"
   },
   "source": [
    "---\n",
    "## Apéndice: fórmulas útiles y tips\n",
    "- `=TRIM()`, `=CLEAN()`, `=PROPER()`, validación de datos (listas).  \n",
    "- `=XLOOKUP(valor, rango_busqueda, rango_resultado, \"NoMatch\", 0)` (coincidencia exacta).  \n",
    "- `=VLOOKUP(valor, tabla, índice, FALSE)` (evita coincidencia aproximada salvo que la necesites).  \n",
    "- **Pivots**: usa “Mostrar valores como % del total” para participación.  \n",
    "- **Documenta**: todo supuesto de join y tratamiento de `NoMatch` en `JOIN_DOC`.\n",
    "\n",
    "\n",
    "## Cierre\n",
    "\n",
    "- Kahoot\n",
    "- Repaso de aprendizajes y próximos pasos\n",
    "\n",
    "## Siguientes Pasos\n",
    "\n",
    "- **Participación continua:** asistir a Co-Learning y a Sprint Focus, y usar los canales de Discord para hacer preguntas.\n",
    "- **Recordatorios:** la grabación y recursos utilizados, se comparten al finalizar la sesión; en caso de necesitar apoyo adicional, agenda un 1:1.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}